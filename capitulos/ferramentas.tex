\chapter{Materiais e Métodos}\label{cap:ferramentas}

Para obter melhor entendimento sobre possíveis ferramentas de visão computacional e detecção facial, este capítulo descreve a execução de testes utilizando a linguagem de programação Python e a implementação da ferramenta OpenCV na mesma linguagem.

\section{OpenCV}

A ferramenta OpenCV, que pode ser encontrada no website Github \cite{itseez2015opencv}, é uma biblioteca de código aberto focada na solução de problemas utilizando visão computacional em tempo real, desenvolvida pela Intel e posteriormente pela Itseez, com suporte a múltiplas plataformas e uso gratuito sobre a licença de código aberto BSD. A ferramenta apresenta suporte a frameworks de aprendizado profundo, como TensorFlow, Pytorch e Caffe e contempla tanto funções básicas, para aplicações como processamento de imagem, alteração de cor ou resolução, até aplicações avançadas, como detecção facial, identificação de características e biometria \cite{wiki:OpenCV}.

Uma das ferramentas que será utilizada neste trabalho é a função de detecção de faces da ferramenta OpenCV, que utiliza um classificador em cascata baseado características. Esse é um método eficiente para reconhecimento de faces em imagens proposto por Paul Viola and Michael Jones, amplamente conhecido como método Viola-Jones, onde uma função é treinada com muitos exemplos positivos (imagens que contém o objeto a ser detectado) e negativos (imagens que não contém o objeto a ser detectado) e então utilizada para detectar as mesmas características em outras imagens \cite{itseez2014theopencv}.

A detecção de faces utilizando OpenCV consiste em duas etapas principais, a primeira consiste no treinamento do modelo, onde são apresentadas diversas imagens já identificadas para que o modelo encontre padrões positivos e negativos. Após o treinamento, a segunda etapa consiste em utilizar o modelo obtido para identificar, em novas imagens, características semelhantes as vistas nas imagens do treinamento. Neste projeto, será utilizado um modelo fornecido em conjunto com a ferramenta OpenCV, já treinado com diversos exemplos de faces frontais.

O procedimento utilizado permite ainda ajuste de parâmetros para a execução do algoritmo de Viola-Jones, neste teste, serão utilizados: o fator de escala, que é o fator pelo qual as dimensões da imagem serão multiplicadas na tentativa de encontrar faces de diferentes tamanhos (quanto menor, maior a chance de encontrar faces) e o número mínimo de vizinhos, que é número mínimo de detecções, após varias iterações, para uma parte da imagem ser considerada uma face (quanto menor, maior a chance de encontrar faces).

\section{Algoritmo Viola-Jones}

O algoritmo Viola-Jones foi publicado em 2001, no paper "Rapid object detection using a boosted cascade of simple features" ~\cite{paper-viola-jones} e é famoso por sua capacidade de detecção de faces com muita velocidade, isso ocorre devido a 3 principais técnicas utilizadas: o cálculo da imagem integral, o algoritmo \textit{AdaBoost} e o classificador em cascata.

\subsection{Imagem Integral} 

A primeira etapa do algoritmo Viola-Jones consiste em transformar a imagem original em uma imagem integral, isto é feito calculando o valor de cada pixel como a soma de todos os pixels que estão acima ou a esquerda do mesmo, como ilustrado na figura~\ref{fig:integral}.

\begin{figure}[htpb]
    \centering
    \caption{Imagem original (esquerda) e imagem integral (direita).}
    \includegraphics[scale=.4]{figs/imagem-integral.png}
    \legend{Fonte: \citeauthoronline{jensen2008implementingviolajones} (\citeyear{jensen2008implementingviolajones})}
    \label{fig:integral}
 \end{figure}

A utilização desta técnica permite calcular facilmente o tamanho de qualquer retângulo formado entre quatro pixels da imagem, conhecendo apenas o valor dos seus cantos, possibilitando assim a análise rápida de diversas partes da imagem. Tal calculo é feito definindo o retângulo a ser análisado e então aplicando a equação \ref{eq:img-integral}.

\begin{figure}[htb]
    \centering
    \caption{Representação da área da imagem a ser analisada.}
    \includegraphics[scale=.4]{figs/imagem-integral-calculo.png}
    \legend{Fonte: \citeauthoronline{jensen2008implementingviolajones} (\citeyear{jensen2008implementingviolajones})}
    \label{fig:integral-calculo}
 \end{figure}

 \begin{equation}\label{eq:img-integral}
    \text{ Soma do retângulo cinza } = D - (B + C) + A
\end{equation}

Com a possibilidade de calcular facilmente a soma dos pixels de um retângulo arbitrário de forma rápida, o algoritmo para detecção pode analisar diversos trechos da imagem, chamados aqui de características, fazendo a comparação de duas ou mais áreas retângulares predefinidas, como os exemplos ilustrado na figura \ref{fig:features}. 

\begin{figure}[htb]
    \centering
    \caption{Alguns exemplos de características retângulares analisadas.}
    \includegraphics[scale=.5]{figs/features.png}
    \legend{Fonte: \citeauthoronline{paper-viola-jones} (\citeyear{paper-viola-jones})}
    \label{fig:features}
 \end{figure}

 O valor final de cada característica é definido pela soma do valor dos pixels sob o retângulo cinza menos a soma do valor dos pixels sob o retângulo branco.

%-
\subsection{Algoritmo de AdaBoost}
%-

As características demonstradas anteriormente, são definidas basicamente como duas ou mais áreas retângulares de qualquer tamanho, tal simplicidade implica na possibilidade da criação de uma enorme variação das mesmas que precisariam ser calculadas diversas vezes, para cada parte de imagem e com diferentes tamanhos, isso implica em um alto custo de processamento, para evitar tal problema, durante a etapa de treinamento do modelo, é utilizado o algoritmo de \textit{AdaBoost} \cite{adaboost-Freund}, que identifica quais são as características com maior probabilidade de acerto.

O \textit{AdaBoost}, que tem seu nome derivado de \textit{adaptative boosting}, é um método de aprendizado de máquina que utiliza a combinação de vários classificadores fracos para obter uma classificação forte, no caso da detecção facial, o algoritmo é utilizado tanto para selecionar um conjunto de características mais eficientes como para treinar o classificador \cite{fabio-luciana-2015}.

\begin{figure}[htb]
    \centering
    \caption{Características retângulares mais eficientes para detecção facial.}
    \includegraphics[scale=.4]{figs/top-features.png}
    \legend{Fonte: \citeauthoronline{paper-viola-jones} (\citeyear{paper-viola-jones})}
    \label{fig:top-features}
 \end{figure}

 A figura \ref{fig:top-features} retrata as melhores características registradas por \citeauthoronline{paper-viola-jones} (\citeyear{paper-viola-jones}), fica claro que as mesmas se destacam por evidenciar as regiões dos olhos e do nariz.

%-
\subsection{Classificador em Cascata}
%-

Pensando que, na maioria dos casos, uma face não ocupa a maior parte de uma imagem a ser identificada, é necessário encontrar uma forma rápida de descartar os elementos do fundo da mesma e concentrar o poder de processamento nos elementos que tem maior probabilidade de serem reconhecidos como uma face, isso leva a uma formulação para o problema onde ao contrário de encontrar faces, é necessário um algoritmo que descarte as "não faces".

Para tal problema, o classificador em cascata apresenta uma ótima solução, esta consiste na utilização de uma série de classificadores que são aplicados de forma sequencial, conforme ilustrado na Figura \ref{fig:cascade-classifier}, permitindo que imagens que certamente não possuem faces sejam rapidamente descartadas logo nas primeiras iterações, enquanto imagens com possíveis faces são classificadas por toda cascata, trazendo um elevado nível de confiança ao resultado.

\begin{figure}[htb]
    \centering
    \caption{Diagrama do funcionamento do classificador em cascata.}
    \includegraphics[scale=.2]{figs/cascade-classifier.png}
    \legend{Fonte: \citeauthoronline{paper-viola-jones} (\citeyear{paper-viola-jones})}
    \label{fig:cascade-classifier}
 \end{figure}

 Um classificador comum, com um único estágio normalmente aceitaria muitos casos de falso negativo, para reduzir a taxa de falsos positivos e de descarte de imagens relevantes, mas no classificador em cascata, falsos positivos nos primeiros estágios não são um problema, pois serão analisados em outros diversos estágios e provavelmente eliminados.

 A utilização desse modelo combinada com o algoritmo \textit{AdaBoost}, possibilita a análise das característica mais eficientes logo no início e consequentemente o descarte muito mais rápido dos casos negativos nos primeiros estágios.

 \section{Conjuntos de imagens para teste}

 Para analisar a eficiência da implementação do algoritmo \textit{Viola-Jones} na biblioteca \textit{OpenCV}, assim como o seu modelo previamente treinado, foram utilizados dois conjuntos de imagens. Tendo em mente o objetivo deste projeto, de identificar rapidamente imagens que não possuem uma face, são tratadas como imagens positivas as imagens que não possuem faces e como negativas as imagens que possuem ao menos uma face.

O primeiro dataset, nomeado \textit{Hotels-50k} \cite{hotels50k-article}, será utilizado para representar imagens da classe negativa que consiste em um conjunto de mais de 50 mil imagens de diversos quartos de hotel vazios, pois imagens com características semelhantes a estas são muitas vezes submetidas erroneamente em cadastros pessoais, ao invés de uma imagem da face a ser cadastrada.

O segundo dataset, nomeado \textit{UTKFace}, disponível em \cite{utkface}, será utilizado para representar imagens da classe positiva que consiste em um conjunto de mais de 20 mil imagens, com uma única face em cada, de diversas pessoas entre 0 e 116 anos de idade, catalogadas de acordo com idade, raça e sexo. Alguns exemplos das imagens contidas no dataset podem ser vistos na figura \ref{fig:exemplos-utk}.

\begin{figure}[htb]
    \centering
    \caption{Exemplos de imagens do dataset \textit{UTKFace}.}
    \includegraphics[scale=.3]{figs/exemplos-utk.png}
    \legend{Fonte: \textit{UTKFace dataset} \cite{utkface})}
    \label{fig:exemplos-utk}
 \end{figure}

Para os testes, foram filtradas apenas imagens de pessoas entre 18 e 60 anos de idade do dataset \textit{UTKFace}, obtendo 17100 imagens e então selecionadas aleatoriamente 4275 imagens do dataset \textit{Hotels-50k}. Assim, foi totalizado um conjunto de 21375 imagens, onde 80\% delas eram positivas (possuíam ao menos uma face) e 20\% negativas (não possuíam faces). Tal proporção representa a hipótese de que cerca de 20\% de imagens enviadas para cadastros na verdade não possuem uma face.

%-
\section{Metodologia de Análise}
%-

Para melhor analisar os resultados dos testes, foi necessário especificar com clareza como poderiam ser agrupadas as imagens, dada a sua origem e o resultado observado no teste, para isso foram utilizadas as definições da Tabela \ref{tab:grupos-images}. 

\begin{table}[htbp]
    \caption{Grupos observados}
    \label{tab:grupos-images}
    \centering
    \begin{tabular}{ccc}\hline\hline
        \textbf{Grupo} & \textbf{Descrição} & \textbf{Quantidade} \\\hline
        \textit{A} & Imagens que contêm uma face & 17130 \\
        $\overline{A}$ & Imagens que não contêm nenhuma face & 4275 \\
        \textit{B} & Imagens onde o algoritmo identificou uma ou mais faces & Variável \\
        $\overline{B}$ & Imagens onde o algoritmo não identificou nenhuma face & Variável \\
    \hline\hline
    \end{tabular}
\end{table}

Definidos os grupos, pode-se utilizar a matriz de confusão da Tabela \ref{tab:matriz_de_confusao} para facilitar a análise da relação entre os grupos definidos anteriormente. Na Tabela \ref{tab:matriz_de_confusao} são observados os grupos \textit{a} (verdadeiro positivo) onde o algoritmo identifica corretamente uma face em cada uma das imagens que realmente contêm uma face, \textit{b} (falso negativo) onde o algoritmo erroneamente não reconheceu nenhuma face, apesar das imagens conterem uma face cada, \textit{c} (falso positivo) onde o algoritmo erroneamente identificou ao menos uma face, mesmo as imagens não contendo nenhuma e \textit{d} (verdadeiro negativo) onde o algoritmo identificou corretamente que não existia nenhuma face nas imagens. É importante destacar que os quatro grupos destacados na matriz de confusão são mutuamente excludentes \cite{Dougherty:2012:PRC:2553126}.

\begin{table}[htbp]
    \caption{Matriz de confusão}
    \label{tab:matriz_de_confusao}
    \centering
    \begin{tabular}{ccc}\hline\hline
        & \textit{B} & $\overline{B}$ \\
    \textit{A} & \textit{a} (verdadeiro positivo) & \textit{b} (falso negativo) \\
    $\overline{A}$ & \textit{c} (falso positivo) & \textit{d} (verdadeiro negativo) \\
    \hline\hline
    \end{tabular}
\end{table}

Para melhor entender os agrupamentos da matriz de confusão, pode-se observar na Figura \ref{fig:norm_dist} as distribuições que representam a quantidade de imagens dos grupos \textit{A} e $\overline{A}$ dada a sua probabilidade de conter uma face. 

\begin{figure}[htbp]
    \centering
    \caption{Exemplo teórico de uma distribuição normal dos grupos de uma matriz de confusão.}
    \includegraphics[scale=.4]{figs/norm_dist.png}
    \label{fig:norm_dist}
 \end{figure}

 Nas distribuições são destacados os grupos \textit{b} (falso negativo) e \textit{c} (falso positivo) e fica evidente que, devido a sobreposição das distribuições \textit{A} e $\overline{A}$, é necessário definir uma limite para decisão, que pode ser ajustado conforme a necessidade, mas que independente do seu ajuste, sempre existirá um grupo categorizado de forma incorreta.

 A matriz de confusão também pode ser escrita em termos probabilísticos (Tabela \ref{tab:matriz_de_confusao_probab}), incluindo as probabilidades marginais ou pode ser visualizada no diagrama de Venn correspondente (Figura \ref{fig:venn_diagram}).

\begin{table}[htbp]
    \caption{Matriz de confusão com probabilidades marginais}
    \label{tab:matriz_de_confusao_probab}
    \centering
    \begin{tabular}{cccc}\hline\hline
        & \textit{B} & $\overline{B}$ & Soma\\
    \textit{A} & $P(A \cap B)$ & $P(A \cap \overline{B})$ & $P(A)$ \\
    $\overline{A}$ & $P(\overline{A} \cap B)$ & $P(\overline{A} \cap \overline{B})$ & $P(\overline{A})$ \\
    Soma & $P(B)$ & $P(\overline{B})$ & 1 \\
    \hline\hline
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \caption{Diagrama de Venn para as imagens analisadas.}
    \includegraphics[scale=.4]{figs/venn-diagram.png}
    \label{fig:venn_diagram}
\end{figure}

Podem também ser calculadas as medidas tradicionais de sensitividade (a probabilidade condicional do algoritmo identificar ao menos uma face dado que a imagem contém uma face) e especificidade (a probabilidade condicional do algoritmo não identificar nenhuma face em uma imagem que realmente não contém nenhuma face) conforme as equações \ref{eq:sensitividade} e \ref{eq:especificidade}.

\begin{align} \label{eq:sensitividade}
    \text{sensitividade, } P(B|A) = P(A \cap B) / (P(A \cap B) + P(A \cap \overline{B})) = a/(a + b) \\
    \label{eq:especificidade}
    \text{especificidade, } P(\overline{B} | \overline{A}) = P(\overline{A} \cap \overline{B}) / (P(\overline{A} \cap \overline{B}) + P(\overline{A} \cap B)) = d/(d + c)
\end{align}

Por fim, a partir da sensitividade e especificidade é possível exibir o resultado do classificador no espaço ROC (\textit{Receiver Operating Characteristic}, traduzido literalmente como Característica operacional do receptor), que consiste em um plano onde o eixo horizontal mede a taxa de falsos positivos (1 - especificidade) e o eixo vertical mede a taxa de verdadeiros positivos (sensitividade), isso permite comparar facilmente diferentes classificadores, pois quanto mais próximo do canto esquerdo superior, melhor a classificação.

\begin{figure}[htbp]
    \centering
    \caption{Espaço ROC.}
    \includegraphics[scale=.4]{figs/curva_roc.png}
    \label{fig:roc_space}
\end{figure}

\section{Análise de Custo}

Para entender o valor da utilização de um modelo de reconhecimento facial para filtragem de imagens inválidas, pode-se utilizar o exemplo de uma empresa de cartões de crédito que precisa analisar fotos de clientes antes de fornecer um cartão para os mesmos. Considerando como R o valor presente líquido \cite{npv-article}, ou seja, a receita total que este cliente traz para empresa, e H como o custo de trabalho humano para a análise de uma imagem, podemos calcular a receita da empresa antes e depois da aplicação de um modelo de detecção facial.

Antes da aplicação do modelo, todas as fotos enviadas deveriam ser analisadas por um humano, gerando o custo de análise manual H, e apenas as fotos que continham uma face trariam a receita R para empresa, relacionando isso aos valores encontrados na matriz de confusão do modelo, onde VP é a taxa de verdadeiros positivos, FP a taxa de falsos positivos, VN a taxa de verdadeiros negativos e FN a taxa de falsos negativos, pode-se obter a equação \ref{eq:g_0}. Nesta equação tem-se o $Evento\:A = (VP + FN)$, que representa os casos onde uma foto que realmente contém uma face, multiplicado pela receita de cada cliente R, subtraídos do custo de análise H multiplicado pela quantidade total de fotos recebidas, que equivale a soma de todas as taxas da matriz de confusão.

\begin{align} \label{eq:g_0}
    g_0 = (VP+FN) \times R - (VP+FP+FN+VN) \times H
\end{align}

Após a aplicação do modelo, apenas as fotos que foram classificadas pelo modelo como faces serão analisadas por humanos, gerando custo de análise manual H e apenas as fotos classificadas pelo modelo como face e que realmente contenham uma face se tornarão clientes reais, trazendo receita R para a empresa, com isso, pode-se obter a equação \ref{eq:g_1}.
\begin{align} \label{eq:g_1}
    g_1 = VP \times R - (VP+FP) \times H
\end{align}

Para avaliar a receita que o modelo traz para a empresa, pode-se calcular a diferença entre a receita antes e depois do modelo, obtendo a equação \ref{eq:profit}.
\begin{align} \label{eq:profit}
    g_1 - g_0 = VN - (\frac{R}{H} - 1) \times FN
\end{align}

Considerando que para o modelo trazer valor para a empresa, é necessário que a receita após sua aplicação seja maior que a receita antes da sua aplicação, obtendo a equação \ref{eq:profit2}.
\begin{align} \label{eq:profit2}
    g_1 - g_0 > 0 \\
    VN - (\frac{R}{H} - 1) \times FN > 0
\end{align}

A equação obtida ainda pode ser reescrita em função da taxa de verdadeiros positivos (TVP) e da taxa de falsos positivos (TFP) de forma que seja possível traça-la sobre o espaço ROC e visualizar de forma simples quais classificadores satisfazem o critério necessário para obtenção de receita. Para isso são consideradas a quantidade de imagens positivas ($ P = 0.8 $) e negativas ($ N = 0.2 $) definidas anteriormente e as relações \ref{eq:fn_tvp} e \ref{eq:vn_tfp}, obtendo por fim a equação \ref{eq:profit_roc} que pode ser exibida no espaço ROC em \ref{fig:profit_roc}.

\begin{align} 
    \label{eq:fn_tvp}
    FN &= P \times (1-TVP) \\
    \label{eq:vn_tfp}
    VN &= N \times (1-TFP) \\
    \label{eq:profit_roc}
    TVP &= 1 + \left( \frac{N}{P \times \left( \frac{R}{H}-1 \right) } \times (TFP - 1) \right)
\end{align}

\begin{figure}[htbp]
    \centering
    \caption{Limiar lucrativo exibido no espaço ROC.}
    \includegraphics[scale=1]{figs/curva_roc_lucro.png}
    \label{fig:profit_roc}
\end{figure}

Assim, é possível avaliar em função da matriz de confusão do modelo, do custo de análise manual de um documento, do valor presente líquido de um cliente e da proporção média de imagens positivas e negativas, se a aplicação deste modelo é viável ou não.
